# deep-learning
More details can be found this link.
[backpropagationを理解しよう](https://qiita.com/okazawaryusuke/private/33d2a454a6ebd1908aee)
This article is how to study and make an experiment deep learning.(Attension:this article is so long)
## neural-net
[neural-net](https://github.com/ryuwryyy/deep-learning/tree/master/neural-net)
* xor
* activation function
* train neuralnet
* mnist

## backprop
[backprop](https://github.com/ryuwryyy/deep-learning/tree/master/backprop)
* AND/XOR gate ver
* layer
* function(add forward/backward)
* TwoLayerNet

## train-tech
[train-tech](https://github.com/ryuwryyy/deep-learning/tree/master/train-tech)
* batch norm
* hyperparameter optimize
* overfit droupout
* overfit weight decay atc...

## CNN
[CNN](https://github.com/ryuwryyy/deep-learning/tree/master/CNN)
* filter apply
* convnet

## common/dataset
[common](https://github.com/ryuwryyy/deep-learning/tree/master/common)
[dataset](https://github.com/ryuwryyy/deep-learning/tree/master/dataset)

## intern-exam
[intern-exam](https://github.com/ryuwryyy/deep-learning/tree/master/intern-exam)
* Newton method
* Pyhton training
* backprop/xor/nn/optimize training
> internship in [m-gram](https://m-gram.com)
